---
title: "Statistical learning og programmering"
author: "Af Kenneth Gottfredsen, Eva Rauff og Sanne Sørensen"
date: "`r Sys.Date()`"
output:
  pdf_document: 
    toc: yes
    df_print: paged
  word_document:
    toc: yes
  html_document:
    toc: yes
    df_print: paged
subtitle: '1. Semesterprojekt. Antal ord:'
header-includes:
- \usepackage{fancyhdr} # Pakke til at justere sidehoved og sidefod.
- \usepackage{lastpage} # Sidetal i højre side af bundmargen.
- \usepackage[danish]{babel} # Alle overskrifter er på Dansk.
latex_engine: xelatex # Xelatex er den gamle engine til TNR.
mainfont: Times New Roman # Times New Roman skrift. 
fontsize: 12pt # Tekststørrelse. 
geometry: margin=1in # Justerer margen.
linestretch: 1.5 # 1.5 linjeafstand. 
documentclass: report # Dokumenttypen.
papersize: a4 # a4 side. 
editor_options: 
  markdown: 
    wrap: sentence
---

<!--- Definerer sidehoved og sidefod --->

```{=tex}
\fancypagestyle{plain}{%
\renewcommand{\headrulewidth}{0pt}%
\fancyhf{}%
\fancyfoot[R]{\footnotesize Side \thepage\,af\,\pageref*{LastPage}}
\setlength\footskip{12.75pt}
}
```
<!--- Justerer sidetypen til plain --->

```{=tex}
\pagestyle{plain}
\pagestyle{fancy}
\thispagestyle{empty}
```
<!--- Sidetal starter fra nul, istedet for 1 --->

\setcounter{page}{0}

<!--- Laver en ny side efter indholdsfortegnelsen --->


```{r, Setup, include=FALSE, eval=TRUE}
knitr::opts_chunk$set(echo = TRUE)
options(tinytex.verbose = TRUE)
```
\newpage

## Problemfelt

## Videnskabsteori og metode

## Fra forretningsproblem til datamining problem

Thise fortæller, at de har svært at forudsige præcist hvor mange liter koldskål produktionsafdelingen skal producere.
Dette har resulteret i, at de ikke har kunne producere nok koldskål i år, fordi flere af butikkerne i området har oplevet at deres kølediske har været tomme for koldskål i sommerperioden.
Den kvantitative del af analysen er afgrænset til COOP butikker i nærheden af Landbohøjskolen, hvis beliggenhed er i Københavnområdet.
Butikkerne afgiver ordre til Thises fjernlager, hvorefter koldskålen bliver leveret ud til butikkerne.

Formålet med analysen er derfor, at beregne en multibel lineær regressionsmodel som bedst kan forudsige butikkernes efterspørgsel på koldskål i området omkring Landbohøjskolen.
Derudover vil vi også finde ud af, hvordan vejret og andre vejr-relateret faktorer påvirker butikkernes efterspørgsel på koldskål.
Med denne fremgangsmåde kan Thise få løst deres forretningsproblem.
Vores datamining problem går ud på, at identificere de forskellige vejr-variablers effekt på efterspørgspørgslen af koldskål.

Dertil vil vi bringe analysens resultater i samspil med Thises grad af datamodenhed, da vi har vurderet virksomhed til, at være i startfasen rent datamodenhedsmæssigt.
Vi vil derfor tilbyde dem en række datainitiativer.
Disse værktøjer kan de bruge til, ikke kun at forudsige efterspørgslen af koldskål, men også i forbindelse med andre varenumre som fx.
Thises græske yoghurt eller skyr fremadrettet.
På denne måde kan Thise mejeri vha.
en øget datamodenhed forøge deres samlede omsætning betydeligt over tid, såfremt de tager datainitiativerne i anvendelse.
I næste afsnit startes der ud med, at importere undersøgelsens datasæt.
\newpage

Først indlæses `pacman::load()`:

```{r, Chunk 1, include=TRUE, eval= TRUE}
# Vi bruger pacman til at installere og indhente relevante
# pakker på samme tid. 
pacman::p_load("tidyverse", "magrittr", "nycflights13", "gapminder",
               "Lahman", "maps", "lubridate", "pryr", "hms", "hexbin",
               "feather", "htmlwidgets", "broom", "pander", "modelr",
               "XML", "httr", "jsonlite", "lubridate", "microbenchmark",
               "splines", "ISLR2", "MASS", "testthat", "leaps", "caret",
               "RSQLite", "class", "babynames", "nasaweather",
               "fueleconomy", "viridis", "readxl", "timeDate", "tinytex",
               "ggbeeswarm", "palmerpenguins", "hms", "RColorBrewer",
               "boot", "openxlsx", "writexl", "PerformanceAnalytics", "car")
```


## Import

I første omgang vil vi importere datasættet:

```{r, Chunk 2, include=TRUE, eval= TRUE}
# Indlæser datasæt og gemmer det nye datasæt i et objekt. 
data1 <- read_excel("data/stud_exam_data.xlsx")
 # Dernæst undersøges strukturen i datasættet.
str(data1)
```

## Tidying og transformering af datasæt

Nu har vi fået indlæst datasættet. Det næste skridt er gøre strukturen i vores dataframe nemmere, at arbejde med og mere læsevenlig. Denne proces kaldes for tidy data, det betyder at hver variabel har en kolonne, hver observation en række, samt hver observationsenhed er i en tabel (Wickham 2022). Dette vil gøre analysearbejdet væsentlig nemmere. Vi starter ud med at rekode nogle af variablerne, så de stemmer overens med hvad der står i opgavebesvarelsen.   

I nedestående kode-chunk rekodes og transformeres de udvalgte variabler så de stemmer overens med eksamensbesvarelsen. Hele kodestumpen vil blive kædet sammen med 'pipe' funktionen' fra dplyr pakken. Omkodningerne bliver til sidst gemt i en ny dataframe som kaldes data1.

Derefter bruges mutate() til at lave en ny kolonne ud fra data1.
Først laves der en date-variabel, som bliver kodet om til et date objekt med `ymd()` fra lubridate pakken.

I den næste del anvendes `mutate()` til, at lave en kolonne der hedder dag, som bliver omkodet til en faktor.
Dernæst koder vi date til et objekt med `ymd()` funktionen fra lubridate pakken.
"lubridate.week.start",1=mandag, istedet for søndag som er standardindstillingerne i R.

Dernæst bruger vi `mutate()` igen til at danne en ny weekend-variabel der hedder weekend_1.
I denne sammenhæng vælger vi at fredag, lørdag, søndag og fire andre helligdage er 1, ellers er de andre værdier 0.
Dette kaldes for en dummyvariabel.

Måned, dag, kamjunk, forvent_lager og weekend_1 er alle kategoriske faktorer.
For at gøre det nemmere at forstå hvad de forskellige værdier udtrykker, navngiver vi disse med `fct_recode()` funktionen.

```{r, Chunk 3 - Transformering, include=TRUE, eval= TRUE}
data1 <- data1 %>%
  mutate(date = ymd(date), måned = factor(month(date)),
         kamjunk = factor(kammerjunkere), forvent_lager =
           factor(forventet_l_lager)) %>%
  mutate(dag = as.factor(wday(date, week_start =
                                getOption("lubridate.week.start", 1)))) %>%
  mutate(weekend_1 = as.integer(dag %in% c("5", "6", "7")| date %in%
                                  ymd("2022-04-14", "2022-04-18", "2022-05-26",
                                      "2022-06-06"))) %>%
  mutate(weekend = factor(weekend_1)) %>%
  mutate(data1, kamjunk = fct_recode(kammerjunkere, "ja" = "0",
                                     "nej" = "1")) %>%
  mutate(data1, forvent_lager = fct_recode(forventet_l_lager, "lav" = "1",
                                           "mellem" = "2", "høj" = "3")) %>%
  mutate(data1, måned = fct_recode(måned, "april" = "4", "maj" = "5",
                                   "juni" = "6", "juli" = "7",
                                   "august" = "8")) %>%
  mutate(data1, dag = fct_recode(dag, "mandag" = "1", "tirsdag" = "2",
                                 "onsdag" = "3", "torsdag" = "4",
                                 "fredag" = "5", "lørdag" = "6",
                                 "søndag" = "7")) %>%
  dplyr::select(date, måned, dag, efterspørgsel, kamjunk, forvent_lager,
                weekend_helligdag = weekend)
```

I denne kodechunk vil vi lave en HTTP GET-anmodning til en API fra DMI.
Vi skal bruge adgangen til at få de relevante vejr-variable som vi senere skal bruge i vores analyse.
API'en leverer til slut et objekt i JSON format som bliver transformeret om til en dataframe i stedet for en liste.

Først bruger vi base_url og info_url til at anmode om vejrdata fra DMI's API.
req_url bruges til at udvælge specifikke parametre fra API´en.

```{r, Chunk 4 - Vejrdata fra DMI, include=FALSE, eval= TRUE}

Base_url <- "https://dmigw.govcloud.dk/v2/"	 
Info_url <- "metObs/collections/observation/items?"
Req_url <-  "stationId=06186&datetime=2022-04-01T12%3A00%3A00Z%2F2022-08-30T12%3A00%3A00Z&api-key=1fda8cc2-25bf-45e3-9f43-702fb9ccfdf3&limit=80182" 
# full_url er en sammenkædning af base_url, info_url og reg_url. 
Full_url <- base::paste0(Base_url, Info_url, Req_url)
# Anmoder om at få adgang til apién vha. httr pakken. 
Api_call <- httr::GET(Full_url) 
# Undersøger status fra apién. 200 indikerer at der ikke er en fejl. 
Api_call$status_code 
# Undersøger hvilket format apikaldet er. Her er det json format. 
http_type(Api_call) 
# Undersøger rådata. 
Api_call$content
# Omdanner rådata til en karakter streng. 
api_char <- base::rawToChar(Api_call$content) 
# Konverterer karakterstrengen til et json objekt. 
api_JSON <- jsonlite::fromJSON(api_char, flatten = TRUE)
# laver en variabel som bliver kædet sammen til et json objekt.
list_dmi <- api_JSON
# Laver et nyt objekt som hedder data2 ud fra list_dmi. Denne liste bliver
# konverteret til en dataframe, så vi kan arbejde videre med vores data.
data2 <- as.data.frame(do.call(cbind, list_dmi))

```

I denne kodechunk vil vi transformere den data vi har hentet fra vores API-kald til nogle mere brugbare data.

Først bruger vi base_url og info_url til at anmode om vejrdata fra DMI's API.
req_url bruges til at udvælge specifikke parametre fra API´en.

Derefter bruger vi `pivot_wider()`-funktionen til at sprede variablerne ud i deres egne separate kolonner.

Vi bruger derefter Mutate-funktionen til at konvertere kolonnen 'målingstidspunkt' til en datoformat Separate-funktionen bruges til at opdele kolonnen 'målingstidspunkt' i to separate kolonner som vi navngiver 'date' og 'time'.

`Filter()`funktionen udvælger rækker, der indeholder de første fire characters: "12:0".

```{r, Chunk 5 - Tidy af vejrdata fra DMI, include=TRUE, eval= TRUE}

data2 <- as.data.frame(do.call(cbind, list_dmi))
data2 <- dplyr::select(data2, features.properties.observed,
                       features.properties.value,
                       features.properties.parameterId) %>% 
  rename(værdi = features.properties.value, parameter =
                 features.properties.parameterId,
  målingstidspunkt = features.properties.observed) %>%
  pivot_wider(names_from = parameter, values_from = værdi) %>% 
  mutate(målingstidspunkt = as_datetime(målingstidspunkt)) %>%
  separate(målingstidspunkt, into = c('date', 'time'), sep = " ") %>% 
  filter(str_sub(time, 1, 4) == "12:0") %>% 
  mutate(date = as_date(date)) %>%
  mutate(time = as_hms(time)) %>% 
  dplyr::select(-(temp_max_past12h:temp_min_past12h))
```

I nedestående kode-chunk merger vi data1 og data2 til data3 for at beholde alle observationer i x.

Vi bruger derefter mutate til at oprette fire nye variabler i data3 kaldet temp_gt25_3\_dage.
`Lag()`-funktionen er brugt til at lave variablerne, som har opfanget forsinkede værdier fra temp1, temp2 og temp3.
Afslutningsvis dannes variablen 'temp_gt25_3\_dage', som måler de dage hvor der har været mere end 3 dage i træk med \>= 25 grader.
Det er en dummyvariabel fordi vi bruger `if_else`. Da vi har et begrænset antal ord og tegn med mellemrum til rådighed, vil vi ikke lave en udtømmende variabelbeskrivelse. Relevante variabler bliver beskrevet når vi tolker på de forskellige parametre i regressionsanalysen.  

```{r, Chunk 6 - Merging af de to datasæt, include=TRUE, eval= TRUE, warning=FALSE}

data3 <- data1 %>%
left_join(data2, data1, by = c("date" = "date"))
dplyr::select(data3, date, time, weekend_helligdag, everything())


data3 <- data3 %>% 
  mutate(temp1 = lag(temp_max_past1h, 1),
         temp2 = lag(temp_max_past1h, 2),
         temp3 = lag(temp_max_past1h, 3),
         temp1 = if_else(is.na(temp1), 0, temp1),
         temp2 = if_else(is.na(temp2), 0, temp2),
         temp3 = if_else(is.na(temp3), 0, temp3),
         temp_gt25_3_dage = if_else(temp1 >= 25 & temp2 >= 25 & temp3 >= 25,
                                    1, 0))
```

## Datavisualisering og eksplorativ analyse 

Nu er vores data blevet gjort tidy, det næste skridt er at undersøge hvilke  vejr-mønstre og tendenser der hænger sammen med butikkernes efterspørgslen af koldskål i det sammenkoblede datasæt, som vi har kaldt data3. Vi går derfor i gang med den eksplorative del af analysen. 

Først identificeres potentielle outliers i vores dataset. Derefter fjerner vi 1 outlier som er 47, da den skiller sig væsentligt ud i forhold til de andre observationer. Umiddelbart vurderes det ikke, at der er mange outliers i vores data som kan have indflydelse på den samlede varians, hvorfor vi kun har fjernet den ene. Tilbage er der n = 151 i vores datasæt.  

Derefter laves der et ggplot for at se fordelingen af efterspørgselen af koldskål i form af simpelt histogram, fordi efterspørgslen er en kontinuert variabel. Det er derfor muligt, at beregne spredningen mellem observationerne.

`geom_density()` funktionen bruges til, at forstå fordelingen og til at forudsige den forventede fordeling af efterspørgslen på koldskål.
Man kan se at at spredningen af observationerne er størst omkring 500.
Endvidere kan det ses, at efterspørgslen af koldskål er tilnærmelsesvis normalfordelt, og at sandsynlighedskurven er symmetrisk klokkeformet.

Dog kan man også se, at nogle af observationerne falder udenfor, hvilket kan skyldes tilfældig variation eller systematiske fejl. 
Man ved derfor, at ca. 50% af observationerne befinder sig til venstre og højre af midten dvs.middelværdien. At vores data er normalfordelt er en fordel, fordi den lineære regressionsmodel som er en parametrisk test, som bla. kræver at data er normaltfordelt. 

```{r, Chunk 7 - Histogram over efterspørgsel, include=TRUE, eval= TRUE, warning=FALSE}

data3 <- data3 %>%
  filter(efterspørgsel > 47)

ggplot(data3, aes(x = efterspørgsel)) +
 geom_histogram(aes(y =..density..), colour = "black",
                fill = "gray", binwidth = 90) +
 geom_density(alpha=0.5, fill="#FF6666", adjust=1.6) +
  labs(title = "Histogram over butikkernes efterspørgsel af koldskål",
       subtitle = "Undersøger om efterspørgslen er normaltfordelt",
       y = "Antal observationer",
       x = "Butikkernes efterspørgsel af koldskål",
       caption = "Kilde: Tal fra DMI 2002. Fra perioden 1/4/22-30/8/22") +
  ggeasy::easy_center_title() + # Centrerer titlen.
  theme( plot.title = element_text(hjust = 0.5, size = 16),
         plot.subtitle = element_text(hjust = 0.5, size = 14),
         plot.caption = element_text(hjust = 0.5, face = "italic", size = 10)) +
  xlim(100, 1000) + ylim(0, 0.0035) + 
theme_gray()

```

I følgende kode-chunk har er der lavet et boxplot til at vise den statistiske variationen ift.butikkens forventede lagerbeholdning og efterspørgslen på koldskål.

Her kan man se at median-efterspørgslen stiger når man går fra høj til lav forventet lagerbeholdning af koldskål. Dette tyder også på at der er en signifikant sammenhæng mellem de 2 variabler. Man kan også se, at der er fx. ved en høj forventet lagerbeholdning er en relativ stor usikkerhed i forhold til mellem og lav lagerbeholdning, dette indikerer at datapunkterne er en del spredt ud.  

```{r, Chunk 8 - Boxplot over lagerbeholdning og efterspørgsel, include=TRUE,eval= TRUE, warning=FALSE}

ggplot(data = data3, mapping = aes(x = forvent_lager, y = efterspørgsel, fill =
                                     forvent_lager)) +
  stat_boxplot(geom = 'errorbar') + # Undersøger usikkerheden og spredningen.  
  geom_boxplot() + 
  labs(title = "Lagerbeholdningen og efterspørgsel af koldskål",
       subtitle = "Variationen ift. den forventede lagerbeholdning og koldskål", 
       caption = "Kilde: Tal fra DMI 2002. Fra perioden 1/4/22-30/8/22",
       y = "Butikkernes efterspørgsel på koldskål (ltr)",
       x = "Butikkens forventede lagerbeholdning") + 
  geom_beeswarm(dodge.width=3, cex = 1, color = "black") + # undgår overplot.
  ggeasy::easy_center_title() +
  theme( plot.title = element_text(hjust = 0.5, size = 16),
         plot.subtitle = element_text(hjust = 0.5, size = 14), 
         plot.caption = element_text(hjust = 1, face = "italic",size = 10 )) +
  scale_fill_brewer(palette = "Pastel2") + 
  theme_gray()
```

I næste kode-chunk er der lavet et boxplot som viser fordelingen af efterspørgslen i forhold til om 25% af butikkerne er løbet tør for kammerjunkere eller ej.
På baggrund af plottet kan man se at hvis butikkerne ikke har kammerjunkere på lageret så falder efterspørgslen. Det betyder at Efterspørgslen på koldskål stiger når de er løbet tør for kammerjunkere.

```{r, Chunk 9 - Boxplot over efterspørgsel og kammerjunkere, include=TRUE, eval= TRUE, warning=FALSE}

ggplot(data = data3, mapping = aes(x = kamjunk, y = efterspørgsel, fill =
                                     kamjunk)) +
  stat_boxplot(geom = 'errorbar') + 
  geom_boxplot() + 
  labs(title = "Kammerjunkere og efterspørgsel af koldskål",
  subtitle = "Variationen ift. den forventede lagerbeholdning af kammerjunker
  og koldskål",
  caption = "Kilde: Tal fra DMI 2002. Fra perioden 1/4/22-30/8/22",
  y = "Butikkernes efterspørgsel på koldskål (L).",
  x = "Mere end 25% af butikkerne er løbet tør for kammerjunkere") +
  ggeasy::easy_center_title() + # Centrerer titlen. 
  geom_beeswarm(dodge.width=3,cex=1, color = "black") + # Justerer boksbredden.
  theme( plot.title = element_text(hjust = 0.5, size = 16),
         plot.subtitle = element_text(hjust = 0.5, size = 14), 
         plot.caption = element_text(hjust = 1.8, face = "italic",
                                     size = 10 )) +
  scale_fill_brewer(palette = "Pastel2") + 
  theme_gray()
```

Forneden har vi lavet et boxplot som viser sammenhængen mellem måned og efterspørgslen af koldskål. Det er tydeligt at se at median-efterspørgslen stiger fra april-juli hvorefter den falder efterspørgslen i august. Denne observation stemmer også overens med påstande fra vores kilder i problemfeltet, og udsagn fra vores interviews med medarbejderne hos Thise Mejeri. Hvilket indikerer at efterspørgselen på koldskål hænger moderat sammen med årstiden, dvs. selve sommerperioden. 

```{r, Chunk 10 - Boxplot over efterspørgsel og måned, include=TRUE, eval= TRUE, warning=FALSE}
ggplot(data = data3, mapping = aes(x = måned, y = efterspørgsel,
                                   fill = måned)) +
  stat_boxplot(geom = 'errorbar') +
  geom_boxplot() + 
  labs(title = "Måned og efterspørgsel af koldskål",
       subtitle = "Variationen ift. måned og efterspørgelsen af koldskål",
       caption = "Kilde: Tal fra DMI 2002. Fra perioden 1/4/22-30/8/22",
       y = "Butikkernes efterspørgsel på koldskål (L).",
       x = "Måned") +
  ggeasy::easy_center_title() + # Centrerer titlen. 
  geom_beeswarm(dodge.width=3,cex = 0.5, color = "black") + # Justerer boksbredden.
  theme( plot.title = element_text(hjust = 1, size = 16),
         plot.subtitle = element_text(hjust = 0.5, size = 14), 
         plot.caption = element_text(hjust = 1.3, face =
                                       "italic", size = 10 )) +
  scale_fill_brewer(palette = "Pastel2") + 
  theme_gray()
```

`predict` 

```{r, Chunk 11 - Scatterplot af temperatur og efterspørgsel, include=TRUE, eval= TRUE, warning=FALSE}
model1 <- lm(efterspørgsel ~ temp_mean_past1h, data = data3)
prædiktion <- predict(model1, interval = "prediction", level = 0.95)
ny_df <- cbind(data3, prædiktion)
ggplot(ny_df, aes(temp_mean_past1h, efterspørgsel)) +
    geom_point() +
    geom_line(aes(y=lwr), color = "red", linetype = "dashed") +
    geom_line(aes(y=upr), color = "red", linetype = "dashed") +
    geom_smooth(method = lm, se = TRUE) +
    labs(title = "Sammenhængen mellem temperatur og efterspørgsel af koldskål",
    subtitle = "Linelær regression der viser relationen mellem den forventede                  lagerbeholdning og koldskål",
    caption = "Kilde: tal fra DMI 2002 fra perioden 1/4/22-30/8/22",
    y = "Butikkernes efterspørgsel på koldskål (ltr.)",
    x = "Maks temperatur hver time (celcius)") +
ggeasy::easy_center_title() + # Centrerer titlen. 
theme( plot.title = element_text(hjust = 0.5, size = 16),
plot.subtitle = element_text(hjust = 0.5, size = 14), 
plot.caption = element_text(hjust = 1, face = "italic", size = 10 ))+
xlim(4.6, 30.8) + ylim(150, 850) + 
theme_gray()
```


## Metodevalg

Den statistiske metode vi vil anvende i denne undersøgelse, kaldes for superviseret metode, fordi den tager udgangspunkt i én afhængig variabel. Den konkrete metode er en multibel lineær regression. Den vil vi anvende til og forudsige efterspørgslen på koldskål i liter fremadrettet på baggrund af effekten af nogle uafhængige vejr-variabler. Når vi først har trænet vores model på træningsdata og fået beregnet de nødvendige koefficienter, får alle variablerne i ligningen smidt en hat på toppen - dette indikerer prædiktion (Hastie et.al 2021). Den generelle formel bliver beskrevet nedenunder:    

$$
 \hat{y} = \hat{\beta_0} + \hat{\beta_1}x_1 + \hat{\beta_2}x_2... + \hat{\beta_p}x_p + \epsilon
$$
$\hat{y}$ er den forudsagte værdi af $Y$ og $\hat{f}$ er et estimat for $f$.  
$\hat{y}$ er desuden den afhængige variabel efterspørgsel i liter. $x_i$ er udvalgte uafhængige variabler. $\hat{y} = f(X)$ indeholder variation som vi kan reducere ved, at bruge den korrekte SL-metode til, at beregne $f$ med. Dog vil det aldrig være en fejlfri model vi ender ud med. Fordi estimatet $\epsilon$ er tilfældige fejl eller støj, man ikke kan gøre noget ved og den type fejl vil altid være til stede (Hastie et.al 2021).


## Modeludvælgelse og test på træningsdata

I dette afsnit vil vi gå i gang med regressionsanalysen. Vi træner først vores model på vores træningsdata, fordi vi gerne vil tilpasse vores modelparametre. Vi bruger træningsdata til, at fintune vores regressionsmodel. Når modellen er blevet trænet godt igennem, bliver den afprøvet på testdata, da vi gerne vil undersøge hvor god modellen er til, at forudsige en så præcis efterspørgslen på koldskål som mulig. Vurderingen af modelpræcisionen bestemmes ud fra den laveste MSE værdi.   



```{r, Chunk 12 - Statistisk analyse, include=TRUE, eval= TRUE, warning=FALSE}
# Tester for samvariation og multikolinearitet på de kontinuerte variabler. 
cor_matrice <- data3 |>  
  dplyr::select(efterspørgsel, 
               temp_min_past1h,
               temp_dry,
               temp_max_past1h,                                                               temp_mean_past1h,
               humidity) 
chart.Correlation(cor_matrice, histogram = TRUE, method = "pearson")
# Der er stærk multikolinearitet, det kan være et problem ift. tolkningen af
# vores multible regressionsmodel. Dette har også en negativ indvirkning på 
# modellens pålidelighed.

```

Eftersom antallet af variabler i vores datasæt er større end antallet af observationer, vil vi bruge backward-selection til, at udvælge de uafhængige variabler der skal med i modellen. Det vil sige, at vi tilføjer alle variable ind på højre side af ligningen, og fjerner dem med den højeste p-værdi indtil der kun er signifikante uafhængige variable tilbage.


```{r, Chunk 13 - Backward selection, include=TRUE, eval= TRUE, warning=FALSE}
lm.fit10 <- lm(efterspørgsel ~., data = data3)
summary(lm.fit10)
lm.fit11 <- lm(efterspørgsel ~ temp_mean_past1h, data = data3)
summary(lm.fit11)
predict(model1,data.frame(temp_mean_past1h = (c(10,20,30))), interval = "prediction", level = 0.95)
```


```{r, Chunk 13 - Statistisk analyse, include=TRUE, eval= TRUE, warning=FALSE}
lm.fit1 = lm(efterspørgsel ~ forvent_lager + weekend_helligdag + kamjunk + temp_gt25_3_dage + I(temp_mean_past1h^1), data = data3)
vif(lm.fit1) # VIF > 1 indikerer at der er inflation i variansen på alle variabler. 
summary(lm.fit1)
#plot(lm.fit1)
# R^2 indikerer at de uafhængige variable forklarer 58% af variansens i data. 
```



```{r, Chunk 14 - Statistisk analyse, include=TRUE, eval= TRUE, warning=FALSE}
attach(data3)
lm.fit1 = lm(efterspørgsel ~ 1) # simpel model
coef(lm.fit1) # Skæringen med y aksen er hvor den gennemsnitlige efterspørgsel
# af koldskål er 520.22 liter. 

lm.fit2 <- lm(efterspørgsel ~ poly(temp_mean_past1h, degree = 3), data = data3)
summary(lm.fit2)

 
lm.fit3 = lm(efterspørgsel ~ temp_mean_past1h + temp_mean_past1h^5) # ekstrem model 
summary(lm.fit3)


#predict(model1,data.frame(temp_mean_past1h), interval = "prediction", level = 0.95)

glm.fit1 = glm(efterspørgsel ~ 1) # simpel model
summary(glm.fit1)

glimpse(data3)
glm.fit2 <- glm(efterspørgsel ~ + forvent_lager + weekend_helligdag + kamjunk + temp_gt25_3_dage + måned + temp_mean_past1h, data = data3) 
cv.err3 <- cv.glm(data3, glm.fit2) # Mellem. 
cv.err3$delta[[1]]
summary(glm.fit2)


glm.fit3 <- glm(efterspørgsel ~ + forvent_lager + weekend_helligdag + kamjunk + temp_gt25_3_dage + måned +  I(temp_mean_past1h^3), data = data3) 
cv.err3 <- cv.glm(data3, glm.fit3) # Kompleks model overfitter, da MSE stiger. 
cv.err3$delta[[1]]
summary(glm.fit3)
```

```{r, Chunk 15 - Statistisk analyse, include=TRUE, eval= FALSE, warning=FALSE}

glimpse(data3)

cv.error <- rep(0, 10)
for (i in 1:10) {
  glm.fit <- glm(efterspørgsel ~ poly(temp_mean_past1h), data = data3)
  cv.error[i] <- cv.glm(data3, glm.fit)$delta[1]
}
cv.error


x <- data3$temp_mean_past1h
y <- data3$efterspørgsel
data <- data.frame(y, x)

ggplot(data3, mapping = aes(x=x, y=y)) + 
  geom_point(alpha=1/3) +
  geom_smooth(method="glm", formula = y ~ poly(x, 1, raw=TRUE), se=FALSE, colour="blue") +
  geom_smooth(method="glm", formula = y ~ poly(x, 3, raw=TRUE), se=FALSE, colour="green") +
  geom_smooth(method="glm", formula = y ~ poly(x, 22, raw=TRUE), se=FALSE, colour="red") +
geom_point(data=data3, mapping = aes(x=x, y=y), alpha=1/3) + 
  labs(title = "Skæringen ved de tre polynomiske regressionsmodeller",
caption = "Kilde: Tal fra DMI 2002 fra perioden 1/4/22-30/8/22",
y = "Butikkernes efterspørgsel på koldskål i Ltr.",
x = "Gennemsnitlige temperatur pr.time i °C.") +
ggeasy::easy_center_title() + # Centrerer titlen. 
theme( plot.title = element_text(hjust = 0.5, size = 16),
plot.subtitle = element_text(hjust = 0.5, size = 14), 
plot.caption = element_text(hjust = 1, face = "italic", size = 10 ))+
xlim(5, 31) + ylim(220, 900) + 
  theme_gray()
```




\newpage

## Tidy

\newpage

## Transformer

\newpage

## Visualiser

\newpage

## Model

\newpage

## Kommunikér/analyse

## Sessioninformation

For at højne gennemsnigtigheden printes der en udskrift om den nuværende R session:

```{r, Chunk 20 - Info om nuværnde R session, include=TRUE, eval= TRUE}
SI <- sessionInfo(package = NULL) # Udskriver en liste om denne R session.  
```

## Litteratur

## Bilag
