---
title: "Statistical learning og programmering"
author: "Af Kenneth Gottfredsen, Eva Rauff og Sanne Sørensen"
date: "`r Sys.Date()`"
output:
  pdf_document: 
    toc: yes
    df_print: paged
  word_document:
    toc: yes
  html_document:
    toc: yes
    df_print: paged
subtitle: '1. Semesterprojekt. Antal ord:'
header-includes:
- \usepackage{fancyhdr} # Pakke til at justere sidehoved og sidefod.
- \usepackage{lastpage} # Sidetal i højre side af bundmargen.
- \usepackage[danish]{babel} # Alle overskrifter er på Dansk.
latex_engine: xelatex # Xelatex er den gamle engine til TNR.
mainfont: Times New Roman # Times New Roman skrift. 
fontsize: 12pt # Tekststørrelse. 
geometry: margin=1in # Justerer margen.
linestretch: 1.5 # 1.5 linjeafstand. 
documentclass: report # Dokumenttypen.
papersize: a4 # a4 side. 
editor_options: 
  markdown: 
    wrap: sentence
---

<!--- Definerer sidehoved og sidefod --->

```{=tex}
\fancypagestyle{plain}{%
\renewcommand{\headrulewidth}{0pt}%
\fancyhf{}%
\fancyfoot[R]{\footnotesize Side \thepage\,af\,\pageref*{LastPage}}
\setlength\footskip{12.75pt}
}
```
<!--- Justerer sidetypen til plain --->

```{=tex}
\pagestyle{plain}
\pagestyle{fancy}
\thispagestyle{empty}
```
<!--- Sidetal starter fra nul, istedet for 1 --->

\setcounter{page}{0}

<!--- Laver en ny side efter indholdsfortegnelsen --->

options(scipen = 100)

```{r, Setup, include=FALSE, eval=TRUE}
knitr::opts_chunk$set(echo = TRUE)
options(tinytex.verbose = TRUE)
```

\newpage

## Problemfelt

## Videnskabsteori og metode

## Metodevalg

Den statistiske metode som anvendes i denne undersøgelse, kaldes for superviseret metode, fordi den tager udgangspunkt i én afhængig variabel.
Den konkrete metode er en multibel lineær regression.
Den vil vi anvende til og forudsige efterspørgslen på koldskål i liter fremadrettet på baggrund af effekten af nogle uafhængige vejr-variabler.
Når vi først har trænet vores model på træningsdata og fået beregnet de nødvendige koefficienter, får alle variablerne i ligningen smidt en hat på toppen - dette indikerer prædiktion (Hastie et.al 2021).
Den generelle formel bliver beskrevet nedenunder:

$$
 \hat{y} = \hat{\beta_0} + \hat{\beta_1}x_1 + \hat{\beta_2}x_2... + \hat{\beta_p}x_p + \epsilon
$$ $\hat{y}$ er den forudsagte værdi af $Y$ og $\hat{f}$ er et estimat for $f$.\

$\hat{y}$ er desuden den afhængige variabel efterspørgsel i liter.
$x_i$ er udvalgte uafhængige variabler.
$\hat{y} = f(X)$ indeholder variation som vi kan reducere ved, at bruge den korrekte SL-metode til, at beregne $f$ med.
Dog vil det aldrig være en fejlfri model vi ender ud med.
Fordi estimatet $\epsilon$ er tilfældige fejl eller støj, man ikke kan gøre noget ved og den type fejl vil altid være til stede (Hastie et.al 2021).

\newpage

Først indlæses `pacman::load()`:

```{r, Chunk 1, include=TRUE, eval= TRUE}
# Vi bruger pacman til at installere og indhente relevante
# pakker på samme tid. 
pacman::p_load("tidyverse", "magrittr", "nycflights13", "gapminder",
               "Lahman", "maps", "lubridate", "pryr", "hms", "hexbin",
               "feather", "htmlwidgets", "broom", "pander", "modelr",
               "XML", "httr", "jsonlite", "lubridate", "microbenchmark",
               "splines", "ISLR2", "MASS", "testthat", "leaps", "caret",
               "RSQLite", "class", "babynames", "nasaweather",
               "fueleconomy", "viridis", "readxl", "timeDate", "tinytex",
               "ggbeeswarm", "palmerpenguins", "hms", "RColorBrewer",
               "boot", "openxlsx", "writexl", "PerformanceAnalytics", "car", "pscl", "caret")
```

## Import

I første omgang vil vi importere datasættet:

```{r, Chunk 2, include=TRUE, eval= TRUE}
# Indlæser datasæt og gemmer det nye datasæt i et objekt. 
data1 <- read_excel("data/stud_exam_data.xlsx")
 # Dernæst undersøges strukturen i datasættet.
str(data1)
```

## Tidying og transformering af datasæt

Nu har vi fået indlæst datasættet.
Det næste skridt er gøre strukturen i vores dataframe nemmere, at arbejde med og mere læsevenlig.
Denne proces kaldes for tidy data, det betyder at hver variabel har en kolonne, hver observation en række, samt hver observationsenhed er i en tabel (Wickham 2022).
Dette vil gøre analysearbejdet væsentlig nemmere.
Vi starter ud med at rekode nogle af variablerne, så de stemmer overens med hvad der står i opgavebesvarelsen.

I nedestående kode-chunk rekodes og transformeres de udvalgte variabler så de stemmer overens med eksamensbesvarelsen.
Hele kodestumpen vil blive kædet sammen med 'pipe' funktionen' fra dplyr pakken.
Omkodningerne bliver til sidst gemt i en ny dataframe som kaldes data1.

Derefter bruges `mutate()` til at lave en ny kolonne ud fra data1.
Først laves der en date-variabel, som bliver kodet om til et date objekt med `ymd()` fra lubridate pakken.

I den næste del anvendes mutate igen til, at lave en kolonne der hedder dag, som bliver omkodet til en faktor.
Dernæst koder vi date til et objekt med `ymd()` funktionen fra lubridate pakken.
"lubridate.week.start",1=mandag, istedet for søndag som er standardindstillingerne i R.

Dernæst bruger vi `mutate()` igen til at danne en ny weekend-variabel der hedder weekend_1.
I denne sammenhæng vælger vi at fredag, lørdag, søndag og fire andre helligdage er 1, ellers er de andre værdier 0.
Dette kaldes for en dummyvariabel.

Måned, dag, kamjunk, forvent_lager og weekend_1 er alle kategoriske faktorer.
For at gøre det nemmere at forstå hvad de forskellige værdier udtrykker, navngiver vi disse med `fct_recode()` funktionen.

```{r, Chunk 3 - Transformering, include=TRUE, eval= TRUE}
data1 <- data1 %>%
  mutate(date = ymd(date), måned = factor(month(date)),
         kamjunk = factor(kammerjunkere), forvent_lager =
           factor(forventet_l_lager)) %>%
  mutate(dag = as.factor(wday(date, week_start =
                                getOption("lubridate.week.start", 1)))) %>%
  mutate(weekend_1 = as.integer(dag %in% c("5", "6", "7")| date %in%
                                  ymd("2022-04-14", "2022-04-18", "2022-05-26",
                                      "2022-06-06"))) %>%
  mutate(weekend = factor(weekend_1)) %>%
  mutate(data1, kamjunk = fct_recode(kammerjunkere, "ja" = "0",
                                     "nej" = "1")) %>%
  mutate(data1, forvent_lager = fct_recode(forventet_l_lager, "lav" = "1",
                                           "mellem" = "2", "høj" = "3")) %>%
  mutate(data1, måned = fct_recode(måned, "april" = "4", "maj" = "5",
                                   "juni" = "6", "juli" = "7",
                                   "august" = "8")) %>%
  mutate(data1, dag = fct_recode(dag, "mandag" = "1", "tirsdag" = "2",
                                 "onsdag" = "3", "torsdag" = "4",
                                 "fredag" = "5", "lørdag" = "6",
                                 "søndag" = "7")) %>%
  dplyr::select(date, måned, dag, efterspørgsel, kamjunk, forvent_lager,
                weekend_helligdag = weekend)
```

I denne kodechunk vil vi lave en HTTP GET-anmodning til en API fra DMI.
Vi skal bruge adgangen til at få de relevante vejr-variable som vi senere skal bruge i vores analyse.
API'en leverer til slut et objekt i JSON format som bliver transformeret om til en dataframe i stedet for en liste.

Først bruger vi base_url og info_url til at anmode om vejrdata fra DMI's API.
req_url bruges til at udvælge specifikke parametre fra API´en.

```{r, Chunk 4 - Vejrdata fra DMI, include=FALSE, eval= TRUE}

Base_url <- "https://dmigw.govcloud.dk/v2/"	 
Info_url <- "metObs/collections/observation/items?"
Req_url <-  "stationId=06186&datetime=2022-04-01T12%3A00%3A00Z%2F2022-08-30T12%3A00%3A00Z&api-key=1fda8cc2-25bf-45e3-9f43-702fb9ccfdf3&limit=80182" 
# full_url er en sammenkædning af base_url, info_url og reg_url. 
Full_url <- base::paste0(Base_url, Info_url, Req_url)
# Anmoder om at få adgang til apién vha. httr pakken. 
Api_call <- httr::GET(Full_url) 
# Undersøger status fra apién. 200 indikerer at der ikke er en fejl. 
Api_call$status_code 
# Undersøger hvilket format apikaldet er. Her er det json format. 
http_type(Api_call) 
# Undersøger rådata. 
Api_call$content
# Omdanner rådata til en karakter streng. 
api_char <- base::rawToChar(Api_call$content) 
# Konverterer karakterstrengen til et json objekt. 
api_JSON <- jsonlite::fromJSON(api_char, flatten = TRUE)
# laver en variabel som bliver kædet sammen til et json objekt.
list_dmi <- api_JSON
# Laver et nyt objekt som hedder data2 ud fra list_dmi. Denne liste bliver
# konverteret til en dataframe, så vi kan arbejde videre med vores data.
data2 <- as.data.frame(do.call(cbind, list_dmi))

```

I denne kodechunk vil vi transformere den data vi har hentet fra vores API-kald til nogle mere brugbare data.

Først bruger vi base_url og info_url til at anmode om vejrdata fra DMI's API.
req_url bruges til at udvælge specifikke parametre fra API´en.

Derefter bruger vi `pivot_wider()`-funktionen til at sprede variablerne ud i deres egne separate kolonner.

Vi bruger derefter Mutate-funktionen til at konvertere kolonnen 'målingstidspunkt' til en datoformat Separate-funktionen bruges til at opdele kolonnen 'målingstidspunkt' i to separate kolonner som vi navngiver 'date' og 'time'.

`Filter()`funktionen udvælger rækker, der indeholder de første fire characters: "12:0".

```{r, Chunk 5 - Tidy af vejrdata fra DMI, include=TRUE, eval= TRUE}

data2 <- as.data.frame(do.call(cbind, list_dmi))
data2 <- dplyr::select(data2, features.properties.observed,
                       features.properties.value,
                       features.properties.parameterId) %>% 
  rename(værdi = features.properties.value, parameter =
                 features.properties.parameterId,
  målingstidspunkt = features.properties.observed) %>%
  pivot_wider(names_from = parameter, values_from = værdi) %>% 
  mutate(målingstidspunkt = as_datetime(målingstidspunkt)) %>%
  separate(målingstidspunkt, into = c('date', 'time'), sep = " ") %>% 
  filter(str_sub(time, 1, 4) == "12:0") %>% 
  mutate(date = as_date(date)) %>%
  mutate(time = as_hms(time)) %>% 
  dplyr::select(-(temp_max_past12h:temp_min_past12h))
```

I nedestående kode-chunk merger vi data1 og data2 til data3 for at beholde alle observationer i x.

Vi bruger derefter mutate til at oprette fire nye variabler i data3 kaldet temp_gt25_3\_dage.
`Lag()`-funktionen er brugt til at lave variablerne, som har opfanget forsinkede værdier fra temp1, temp2 og temp3.
Afslutningsvis dannes variablen 'temp_gt25_3\_dage', som måler de dage hvor der har været mere end 3 dage i træk med \>= 25 grader.
Det er en dummyvariabel fordi vi bruger `if_else`.
Da vi har et begrænset antal ord og tegn med mellemrum til rådighed, vil vi ikke lave en udtømmende variabelbeskrivelse.
Relevante variabler bliver beskrevet når vi tolker på de forskellige parametre i regressionsanalysen.

```{r, Chunk 6 - Merging af de to datasæt, include=TRUE, eval= TRUE, warning=FALSE}

data3 <- data1 %>%
left_join(data2, data1, by = c("date" = "date"))
dplyr::select(data3, date, time, weekend_helligdag, everything())


data3 <- data3 %>% 
  mutate(temp1 = lag(temp_max_past1h, 1),
         temp2 = lag(temp_max_past1h, 2),
         temp3 = lag(temp_max_past1h, 3),
         temp1 = if_else(is.na(temp1), 0, temp1),
         temp2 = if_else(is.na(temp2), 0, temp2),
         temp3 = if_else(is.na(temp3), 0, temp3),
         temp_gt25_3_dage = if_else(temp1 >= 25 & temp2 >= 25 & temp3 >= 25,
                                    1, 0))
```

## Datavisualisering og eksplorativ analyse

Nu er vores data blevet gjort tidy, det næste skridt er at undersøge hvilke vejr-mønstre der hænger sammen med butikkernes efterspørgslen af koldskål i det sammenkoblede datasæt, som vi har kaldt data3.
Vi går derfor i gang med den eksplorative del af analysen.

Først identificeres potentielle outliers i vores dataset.
Derefter fjerner vi 1 outlier som er 47, da den skiller sig væsentligt ud i forhold til de andre observationer.
Umiddelbart vurderes det ikke, at der er mange outliers i vores data som kan have indflydelse på den samlede varians, hvorfor vi kun har fjernet den ene.
Tilbage er der n = 151 i vores datasæt.

Derefter laves der et ggplot for at se fordelingen af efterspørgselen af koldskål i form af simpelt histogram, fordi efterspørgslen er en kontinuert variabel.
Det er derfor muligt, at beregne spredningen mellem observationerne.

`geom_density()` funktionen bruges til, at forstå fordelingen og til at forudsige den forventede fordeling af efterspørgslen på koldskål.
Man kan se at at spredningen af observationerne er størst omkring 500.
Endvidere kan det ses, at efterspørgslen af koldskål er tilnærmelsesvis normalfordelt, og at sandsynlighedskurven er symmetrisk klokkeformet.

Dog kan man også se, at nogle af observationerne falder udenfor, hvilket kan skyldes tilfældig variation eller systematiske fejl.
Man ved derfor, at ca.
50% af observationerne befinder sig til venstre og højre af midtpunktet, dette er middelværdien.
At vores data er normalfordelt er en fordel, fordi den lineære regressionsmodel er en parametrisk test, hvor kravet er at data er normaltfordelt.

```{r, Chunk 7 - Histogram over efterspørgsel, include=TRUE, eval= TRUE, warning=FALSE}

data3 <- data3 %>%
  filter(efterspørgsel > 47) # Fjerner obs. 47 fra datasættet. 

ggplot(data3, aes(x = efterspørgsel)) +
 geom_histogram(aes(y = ..density..), color = "black",
                fill = "grey", binwidth = 90) +
 geom_density(alpha = 0.5, fill="#FF6666", adjust = 1.6) +
  labs(title = "Histogram over butikkernes efterspørgsel på koldskål i ltr.",
       subtitle = "Undersøger om efterspørgslen er normaltfordelt",
       y = "Antal observationer",
       x = "Butikkernes efterspørgsel af koldskål",
       caption = "Kilde: Tal fra DMI 2002. Fra perioden 1/4/22-30/8/22") +
  ggeasy::easy_center_title() + # Centrerer titlen.
  theme( plot.title = element_text(hjust = 0.5, size = 16),
         plot.subtitle = element_text(hjust = 0.5, size = 14),
         plot.caption = element_text(hjust = 1, face = "italic", size = 10)) +
  xlim(100, 1000) + ylim(0, 0.0035)

```

I følgende kode-chunk har er der lavet et boxplot til at vise den statistiske variationen ift.butikkens forventede lagerbeholdning og efterspørgslen på koldskål.

Her kan man se at median-efterspørgslen stiger når man går fra høj til lav forventet lagerbeholdning af koldskål.
Dette tyder også på at der er en signifikant sammenhæng mellem de 2 variabler.
Man kan også se, at der er fx.
ved en høj forventet lagerbeholdning er en relativ stor usikkerhed i forhold til mellem og lav lagerbeholdning, dette indikerer at datapunkterne er en del spredt ud.

```{r, Chunk 8 - Boxplot over lagerbeholdning og efterspørgsel, include=TRUE,eval= TRUE, warning=FALSE}

ggplot(data = data3, mapping = aes(x = forvent_lager, y = efterspørgsel, fill =
                                     forvent_lager)) +
  stat_boxplot(geom = 'errorbar') + # Undersøger usikkerheden og spredningen.  
  geom_boxplot() + 
  labs(title = "Lagerbeholdningen og efterspørgsel af koldskål",
       subtitle = "Variationen ift. den forventede lagerbeholdning og koldskål", 
       caption = "Kilde: Tal fra DMI 2002. Fra perioden 1/4/22-30/8/22",
       y = "Butikkernes efterspørgsel på koldskål i ltr.",
       x = "Butikkens forventede lagerbeholdning") + 
  geom_beeswarm(dodge.width=3, cex = 1, color = "black") + # undgår overplot.
  ggeasy::easy_center_title() +
  theme( plot.title = element_text(hjust = 0.5, size = 16),
         plot.subtitle = element_text(hjust = 0.5, size = 14), 
         plot.caption = element_text(hjust = 2.4, face = "italic",size = 10 )) +
  scale_fill_brewer(palette = "Pastel2")
```

I næste kode-chunk er der lavet et boxplot som viser fordelingen af efterspørgslen i forhold til om 25% af butikkerne er løbet tør for kammerjunkere eller ej.

På baggrund af plottet kan man se at hvis butikkerne ikke har kammerjunkere på lageret så falder efterspørgslen.
Det betyder at Efterspørgslen på koldskål stiger når de er løbet tør for kammerjunkere.

```{r, Chunk 9 - Boxplot over efterspørgsel og kammerjunkere, include=TRUE, eval= TRUE, warning=FALSE}

ggplot(data = data3, mapping = aes(x = kamjunk, y = efterspørgsel, fill =
                                     kamjunk)) +
  stat_boxplot(geom = 'errorbar') + 
  geom_boxplot() + 
  labs(title = "Kammerjunkere og efterspørgsel af koldskål",
  subtitle = "Den forventede lagerbeholdning af kammerjunker
  og koldskål",
  caption = "Kilde: Tal fra DMI 2002. Fra perioden 1/4/22-30/8/22",
  y = "Butikkernes efterspørgsel på koldskål i ltr.",
  x = "Mere end 25% af butikkerne er løbet tør for kammerjunkere") +
  ggeasy::easy_center_title() + # Centrerer titlen. 
  geom_beeswarm(dodge.width=3,cex=1, color = "black") + # Justerer boksbredden.
  theme( plot.title = element_text(hjust = 0.5, size = 16),
         plot.subtitle = element_text(hjust = 0.5, size = 14), 
         plot.caption = element_text(hjust = 1.7, face = "italic",
                                     size = 10 )) +
  scale_fill_brewer(palette = "Pastel2")
```

Forneden har vi lavet et boxplot som viser sammenhængen mellem måned og efterspørgslen af koldskål.
Det er tydeligt at se at median-efterspørgslen stiger fra april-juli hvorefter den falder efterspørgslen i august.

Denne observation stemmer også overens med påstande fra vores kilder i problemfeltet, og udsagn fra vores interviews med medarbejderne hos Thise Mejeri.
Hvilket indikerer at efterspørgselen på koldskål hænger moderat sammen med årstiden, dvs.
selve sommerperioden.

```{r, Chunk 10 - Boxplot over efterspørgsel og måned, include=TRUE, eval= TRUE, warning=FALSE}
ggplot(data = data3, mapping = aes(x = måned, y = efterspørgsel,
                                   fill = måned)) +
  stat_boxplot(geom = 'errorbar') +
  geom_boxplot() + 
  labs(title = "Måned og efterspørgsel af koldskål",
       subtitle = "Variationen ift. måned og efterspørgelsen af koldskål",
       caption = "Kilde: Tal fra DMI 2002. Fra perioden 1/4/22-30/8/22",
       y = "Butikkernes efterspørgsel på koldskål i ltr.",
       x = "Måned") +
  ggeasy::easy_center_title() + # Centrerer titlen. 
  geom_beeswarm(dodge.width = 3,cex = 0.5, color = "black") + # Justerer boksbredden.
  theme( plot.title = element_text(hjust = 1, size = 16),
         plot.subtitle = element_text(hjust = 0.5, size = 14), 
         plot.caption = element_text(hjust = 1.3, face =
                                       "italic", size = 10 )) +
  scale_fill_brewer(palette = "Pastel2")
```

I nedestående kodechunk er der udvalgt en række kontinuerte variabler, fordi ønskes er, at undersøge om disse korrelerer med hinanden, og om deres indbyrdes korrelation er statistisk signifikant.
Der anvendes en `chart.Correlation()` funktion til, at foretage en korrelationsanalyse.

Efterspørgel og humidity er ikke korreleret med hinanden, og dermed ikke statistisk signifikant.
Beslutningen er derfor, at humidity ikke vil blive inkluderet i analysen.
Efterspørgsel og den gennemsnitlige temperatur per time har en korrelations koefficient på 0.38.
P-værdien er meget lav med tre stjerner, det betyder at sammenhængen er signifikant, og det er derfor usandsynligt at opnå et mere ekstremt resultat, hvis man indsamlede en ny stikprøve igen og igen.

Alle temperatur variablerne er tæt på 1, hvilket betyder at de har stærk samvariation.
Dette kaldes for multikolinearitet.
Det vil sige, hvis de blev brugt i den endelige model ville det være vanskeligt, at fortolke på koefficienterne.
Fordi en Model med høj multikolinearitet bliver mindre præcis og mindre pålidelig.

```{r, Chunk 11 - Korrelationsmatrice, include=TRUE, eval= TRUE, warning=FALSE}

cor_matrice <- data3 %>%   
  dplyr::select(efterspørgsel, 
               temp_min_past1h,
               temp_dry,
               temp_max_past1h,                                                               temp_mean_past1h,
               humidity) 
chart.Correlation(cor_matrice, histogram = TRUE, method = "pearson")
```

Som førnævnt var der en moderat signifikant sammenhæng mellem efterspørgslen og gennemsnits-temperaturen.
Derfor har vi valgt at bruge denne variabel som den uafhængige effekt i næste kodechunk.
Som førnævnt var der en moderat signifikant sammenhæng mellem efterspørgslen og gennemsnits-temperaturen.

Efterspørgslen af koldskål bliver prædiktet ud fra den gennemsnitlige temperatur hver time i °C.
Først ved 10, 20 og 30 grader.
Den grå linje omkring tendenslinjen referer til konfidensintervallet af den gennemsnitlige efterspørgsel på koldskål ved en given temperatur.
Mange af observationerne er placeret udenfor dette bånd, hvorfor det er besluttet at anvende et prædiktionsinterval i stedet - der er den røde stiplede linje.
Formålet er med andre ord, at indfange usikkerheden omkring de individuelle værdier og ikke usikkerheden omkring gennemsnittet.

Når den gennemsnitlige temperatur hver time er 10 °C, forudsiger vi at efterspørgslen på koldskål for én ny observation være 440.64 liter.
Ved samme temperatur vil efterspørgslen med 95% sikkerhed være [181.78:699.51].

Når den gennemsnitlige temperatur hver time er 20 °C, forudsiger vi at efterspørgslen på koldskål for én ny observation være 535.25 liter.
Ved samme temperatur vil butikkernes efterspørgslen af koldskål med 95% sikkerhed være [278.23:792.28].

Når den gennemsnitlige temperatur hver time er 30 °C, forudsiger vi at efterspørgslen på koldskål for én ny observation være 629.87 liter.
Ved samme temperatur vil butikkernes efterspørgslen af koldskål med 95% sikkerhed være [369.30:890.43].

Man kan på baggrund af ovenstående tydeligt se, at når den gennemsnitlige temperatur i °C stiger, så stiger butikkernes efterspørgsel på koldskål tilsvarende.

```{r, Chunk 12 - Scatterplot af temperatur og efterspørgsel, include=TRUE, eval= TRUE, warning=FALSE}
model1 <- lm(efterspørgsel ~ temp_mean_past1h, data = data3)
predict(model1,data.frame(temp_mean_past1h = (c(10,20,30))), interval = "prediction", level = 0.95)
prædiktion <- predict(model1, interval = "prediction", level = 0.95)
ny_df <- cbind(data3, prædiktion)
ggplot(ny_df, aes(temp_mean_past1h, efterspørgsel)) +
    geom_point() +
    geom_line(aes(y=lwr), color = "red", linetype = "dashed") +
    geom_line(aes(y=upr), color = "red", linetype = "dashed") +
    geom_smooth(method = lm, se = TRUE) +
    labs(title = "Temperatur og efterspørgsel af koldskål",
    caption = "Kilde: tal fra DMI 2002 fra perioden 1/4/22-30/8/22",
    y = "Butikkernes efterspørgsel på koldskål i ltr",
    x = "Gennemsnitlige temperatur hver time i °C") +
ggeasy::easy_center_title() + # Centrerer titlen. 
theme( plot.title = element_text(hjust = 0.5, size = 16),
plot.subtitle = element_text(hjust = 0.5, size = 10), 
plot.caption = element_text(hjust = 1, face = "italic", size = 10 ))+
xlim(4.6, 30.8) + ylim(150, 850)
```

Træning på træningsdata

Da antallet af variabler i det samlede datasæt er mindre end antallet af observationer, benyttes backward-selection til, at udvælge de uafhængige variabler som fremadrettet skal indgå i modellerne.
Det vil sige, at vi tilføjer alle variable ind på højre side af ligningen, og fjerner dem med den højeste p-værdi indtil der kun er signifikante uafhængige variable tilbage (Hastie et.al 2021).

Vi træner først vores model på vores træningsdata, fordi vi gerne vil tilpasse modelparametrene.
Vi bruger træningsdata til, at fintune vores regressionsmodel.
Når modellen er blevet trænet godt igennem, bliver den afprøvet på testdata, da vi gerne vil undersøge hvor god modellen er til, at forudsige en så præcis efterspørgsel på koldskål som mulig.
Vurderingen af modelpræcisionen bestemmes ud fra den laveste MSE værdi.
MSE måler hvor langt den forudsagte værdi for en observation er fra den faktiske værdi for en observation.
Er MSE lille er der den forudsagte værdi tæt på den faktiske værdi, er MSE stor er den forudsagte værdi langt fra den faktiske værdi.
MSE er således et udtryk for, hvor præcis den udvalgte model er til, at forudsige efterspørgslen af koldskål (Hastie et.al 2021).

Baseline modellen er den simpleste uden uafhængige variabler.
Her er den gennemsnitlige efterspørgsel den afhængige variabel.
Baselinemodellen har MSE = $19.376$.
Det er meget langt fra 0.
Den simple model har en MSE på $16.576$.
Den er bedre end baselinemodellen fordi vi har øget kompleksiteten ved, at tilføje temp_mean_past1h.

```{r, Chunk 13 - Træning, include=TRUE, eval= TRUE, warning=FALSE}
# Baseline model

lm.fit_træning <- lm(efterspørgsel ~ 1, data = data3) # Baseline model
lm_fit1.1_summary <- summary(lm.fit_træning)
mean(lm_fit1.1_summary$residuals^2) # MSE 19376.55
rmse(lm.fit_træning, data = data3)  # RMSE = 139.19
lm.fit_træning

# Simpel model

lm.fit2_træning <- lm(efterspørgsel ~ temp_mean_past1h, data = data3)
lm_fit2_summary <- summary(lm.fit2_træning)
mean(lm_fit2_summary$residuals^2) #  MSE = 16576.45
rmse(lm.fit2_træning, data = data3)  # RMSE = 128.74
lm_fit2_summary

# Mellem model 

lm.fit3_træning = lm(efterspørgsel ~ forvent_lager + weekend_helligdag + kamjunk + temp_gt25_3_dage + måned + I(temp_mean_past1h), data = data3)
lm_fit3_summary <- summary(lm.fit3_træning)
mean(lm_fit3_summary$residuals^2) # MSE = 6740.342
rmse(lm.fit3_træning, data = data3) # RMSE = 82.09959 
lm_fit3_summary

# Kompleks model 

lm.fit4_træning = lm(efterspørgsel ~ forvent_lager + weekend_helligdag + kamjunk + temp_gt25_3_dage + måned + I(temp_mean_past1h^22), data = data3)
lm_fit4_summary <- summary(lm.fit4_træning)
mean(lm_fit4_summary$residuals^2) # MSE = 6923.087
rmse(lm.fit4_træning, data = data3) # RMSE = 83.20509
lm_fit4_summary
```

Test på testdata

I fornævnte afsnit blev den gennemsnitlige MSE beregnet for hver af de fire modeller på træningsdata.
Vi er egentlig ligeglade med disse MSE værdier, det er mere interessant at se hvor præcise forudsigelserne er på testdata.
Træningsdata anvendes som førnævnt til, at udvælge signifikante uafhængige variable og tilpasse modelparametrene.

Dog er det værd at nævne, at den data undersøgelsen er baseret på er simuleret.
Det vil sige, at $f$ på forhånd er kendt.
Den virkelige og yderst komplekse sandhed om efterspørgslen af koldskål ved vi ikke.
Men hvis der bliver udtrukket nogle testdata ud fra data3, kan man validere hvor godt modellerne performer på disse testdata, såfremt modelkompleksiteten øges.
Kompleksiteten kan øges ved, at de kontinuerte variable opløftes i flere potenser, eller ved og inkludere flere uafhængige variabler i modellerne.

Man kan producere testdata på flere måder.
Der anvendes en LOOCV metode, fordi data3 kun indeholder 151 observationer i alt.
Det gode ved denne fremgangsmåde er, at den træner på alle datapunkterne undtagen ét datapunkt.
Processen gentages i dette tilfælde 150 gange.
Derefter beregnes en gennemsnitlig score er beregnet, denne udtrykker hvor god modellen klare sig (dvs. modellen med lavest MSE vælges) (Hastie et.al 2021).

Problemet med metoden er, at den kræver stor computerkraft.
Det tog denne bærbare computer ca.
2 minutter hver gang koden stumpen blev kørt.
Det skyldes, at modellen bliver trænet k gange (ibid).

Modeludvælgelse og test på testdata

+-----------------------+----------------+----------------+----------------+-----------------+
|                       | Baseline       | Simpel         | Mellem         | Kompleks        |
+:======================+:==============:+:==============:+:==============:+:===============:+
| Forvent_lager(mellem) |                |                | $-76.57$\*\*\* | $-74.55$\*\*\*  |
|                       |                |                |                |                 |
|                       |                |                | {$19.82$}      |                 |
+-----------------------+----------------+----------------+----------------+-----------------+
| Forvent_lager(høj)    |                |                | $-82.67$\*\*\* | $-87.00$\*\*\*  |
|                       |                |                |                |                 |
|                       |                |                | {$22.58$}      |                 |
+-----------------------+----------------+----------------+----------------+-----------------+
| Weekend_helligdag(ja) |                |                | $113.01$\*\*\* | $107.33$\*\*\*  |
|                       |                |                |                |                 |
|                       |                |                | {$15.00$}      |                 |
+-----------------------+----------------+----------------+----------------+-----------------+
| Kamjunk(nej)          |                |                | $-71.89$\*\*\* | $-76.52$\*\*\*  |
|                       |                |                |                |                 |
|                       |                |                | {$17.50$}      |                 |
+-----------------------+----------------+----------------+----------------+-----------------+
| Temp_gt25_3\_dage     |                |                | $-82.00$\*     | $-66.74$\*      |
|                       |                |                |                |                 |
|                       |                |                | {$34.23$}      |                 |
+-----------------------+----------------+----------------+----------------+-----------------+
| Måned(maj)            |                |                | $84.37$\*\*\*  | $101.69$\*\*\*  |
|                       |                |                |                |                 |
|                       |                |                | {$24.54$}      |                 |
+-----------------------+----------------+----------------+----------------+-----------------+
| Måned(juni)           |                |                | $129.51$\*\*\* | $162.71$\*\*\*  |
|                       |                |                |                |                 |
|                       |                |                | {$32.79$}      |                 |
+-----------------------+----------------+----------------+----------------+-----------------+
| Måned(juli)           |                |                | $155.95$\*\*\* | $155.947$\*\*\* |
|                       |                |                |                |                 |
|                       |                |                | {$32.79$}      |                 |
+-----------------------+----------------+----------------+----------------+-----------------+
| Måned(august)         |                |                | $85.10$\*      | $197.44$\*      |
|                       |                |                |                |                 |
|                       |                |                | {$34.76$}      |                 |
+-----------------------+----------------+----------------+----------------+-----------------+
| Temp_mean_past1h      |                | $9.46$\*\*\*   | $4.249$\*      | $-0.0$          |
|                       |                |                |                |                 |
|                       |                | {$1.89$}       | {$2.07$}       |                 |
+-----------------------+----------------+----------------+----------------+-----------------+
| Uafhængige variable   | $0$            | $1$            | $6$            | $6$             |
+-----------------------+----------------+----------------+----------------+-----------------+
| Skæring               | $520.23$\*\*\* | $346.04$\*\*\* | $379.93$\*\*\* | $434.78$\*\*\*  |
+-----------------------+----------------+----------------+----------------+-----------------+
| Model P-værdi         | $***$          | $***$          | $***$          | $***$           |
+-----------------------+----------------+----------------+----------------+-----------------+
| MSE_træning           | $139.20$       | $128.74$       | $82.10$        | $83.21$         |
+-----------------------+----------------+----------------+----------------+-----------------+
| MSE_test              | $139.20$       | $130.36$       | $88.55$        | $89.37$         |
+-----------------------+----------------+----------------+----------------+-----------------+
| R\^2                  |                | $0.15$         | $0.65$         | $0.64$          |
+-----------------------+----------------+----------------+----------------+-----------------+
| RSE                   | $139.7$        | $129.6$        | $85.26$        | $86.41$         |
+-----------------------+----------------+----------------+----------------+-----------------+
| Obs                   | $151$          | $151$          | $151$          | $151$           |
+-----------------------+----------------+----------------+----------------+-----------------+

Tabel 1.
Summeret modelreferat fra testdata.
Referencegrupper () for faktorerne er: kamjunkja, forvent_lagerlav, månedapril.
{} referer til standardfejlen.
Note:$* = P < 0.1; ** = P < 0.05; *** = P <0.01$

```{r, Chunk 14 - Statistisk analyse, include=TRUE, eval= TRUE, warning=FALSE}
# Baseline model

lm.fit_træning <- lm(efterspørgsel ~ 1, data = data3) # Baseline model
lm_fit1.1_summary <- summary(lm.fit_træning)
mean(lm_fit1.1_summary$residuals^2) # MSE 19376.55
rmse(lm.fit_træning, data = data3)  # RMSE = 139.19
lm.fit_træning

# Simpel model

ctrl <- trainControl(method = "LOOCV") # Udvælger cross-validation metode
model1_test <- train(efterspørgsel ~ temp_mean_past1h, data = data3, method = "lm", trControl = ctrl)
model1_test # RMSE 130.36
summary(model1_test)

# Mellem model

model2_test <- train(efterspørgsel ~ forvent_lager + weekend_helligdag + kamjunk + temp_gt25_3_dage + måned + temp_mean_past1h, data = data3, method = "lm", trControl = ctrl)
model2_test # RMSE 88.55
summary(model2_test)

# Kompleks model

model3_test <- train(efterspørgsel ~ forvent_lager + weekend_helligdag + kamjunk + temp_gt25_3_dage + måned + I(temp_mean_past1h^3), data = data3, method = "lm", trControl = ctrl)
model3_test # RMSE 89.61
summary(model3_test)
```

På baggrund af vores MSE værdier, har vi opstillet et histogram der visuelt viser, hvordan MSE'en bliver mindre i takt med at modelkompleksiteten stiger.
MSE er somsagt et udtryk for hvor godt en model yder på træningsdata.
Men den MSE der bliver beregnet ud fra træningsdata kan være biased, derfor beregnes der en MSE på testdata vha.
LOOCV metoden.
Nævn bias variance-tradeoff her.

```{r, Chunk 15 - Poly plot, include=TRUE, eval= FALSE, warning=FALSE}
x <- data3$temp_mean_past1h
y <- data3$efterspørgsel
data <- data.frame(y, x)

ggplot(data3, mapping = aes(x=x, y=y)) + 
  geom_point(alpha=1/3) +
  geom_smooth(method="glm", formula = y ~ poly(x, 1, raw=TRUE), se=FALSE, colour="blue") +
  geom_smooth(method="glm", formula = y ~ poly(x, 3, raw=TRUE), se=FALSE, colour="green") +
  geom_smooth(method="glm", formula = y ~ poly(x, 22, raw=TRUE), se=FALSE, colour="red") +
geom_point(data=data3, mapping = aes(x=x, y=y), alpha=1/3) + 
  labs(title = "Skæringen ved de tre polynomiske regressionsmodeller",
caption = "Kilde: Tal fra DMI 2002 fra perioden 1/4/22-30/8/22",
y = "Butikkernes efterspørgsel på koldskål i Ltr.",
x = "Gennemsnitlige temperatur pr.time i °C.") +
ggeasy::easy_center_title() + # Centrerer titlen. 
theme( plot.title = element_text(hjust = 0.5, size = 16),
plot.subtitle = element_text(hjust = 0.5, size = 14), 
plot.caption = element_text(hjust = 1, face = "italic", size = 10 ))+
xlim(5, 31) + ylim(220, 900) + 
  theme_gray()
```

```{r, Chunk 16 - Sammenligning af MSE, include=FALSE, eval= FALSE, warning=FALSE}

Metode_LOOCV = c("Træningsdata", "Træningsdata", "Træningsdata", "Testdata", "Testdata", "Testdata")
Model_kompleksitet = c("Simpel", "Mellem", "Kompleks", "Simpel", "Mellem", "Kompleks")
MSE = c(128.7495, 82.09959, 83.20509, 130.36, 88.55084, 89.37393)
test_frame = data.frame(Metode_LOOCV, Model_kompleksitet, MSE,
                            stringsAsFactors = TRUE)
test_frame$Model_kompleksitet <- factor(test_frame$Model_kompleksitet,
                      levels = c("Simpel", "Mellem", "Kompleks"))

test_frame


MSE_plot <- ggplot(test_frame) + 
geom_bar(aes(x = Model_kompleksitet, y = MSE, fill=Metode_LOOCV), 
stat = "identity", # Ikke transformer data. 
position = "dodge") + 
labs(title = "Træning og test MSE",
subtitle = "Forskel i MSE ud fra modelkompleksitet") + 
ggeasy::easy_center_title() + 
theme( plot.title = element_text(hjust = 0.5, size = 16),
plot.subtitle = element_text(hjust = 0.5, size = 10), 
plot.caption = element_text(hjust = 1, face = "italic", size = 10 ))
MSE_plot

#test_frame
#str(test_frame)
#levels(test_frame$Kompleksitet)
#attributes(test_frame)
#glimpse(test_frame)

```

\newpage

## Tidy

\newpage

## Transformer

\newpage

## Visualiser

\newpage

## Model

\newpage

## Kommunikér/analyse

## Sessioninformation

For at højne gennemsnigtigheden printes der en udskrift om den nuværende R session:

```{r, Chunk 20 - Info om nuværnde R session, include=TRUE, eval= TRUE}
SI <- sessionInfo(package = NULL) # Udskriver en liste om denne R session.  
```

## Litteratur

## Bilag
